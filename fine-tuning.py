# -*- coding: utf-8 -*-
"""Copy of Em-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qMb7KoBb3ODhzBT5Idr9DiSmNiBTFkmJ
"""

!pip install unsloth

!pip install torch
!pip install datasets
!pip install trl

import pandas as pd

# Load the uploaded dataset
file_path = "disease_diagnosis.csv"
df = pd.read_csv(file_path)

# Basic EDA summary
eda_summary = {
    "Head": df.head(),
    "Info": df.info(),
    "Description": df.describe(),
    "Null Values": df.isnull().sum(),
    "Unique Values": df.nunique(),
    "Data Types": df.dtypes
}

# Show the first 5 rows separately for visual clarity
df.head()

!pip install cuda-python

!3rd new one(Latest finetuning)
from unsloth import FastLanguageModel
import torch
from datasets import Dataset
import pandas as pd

# Load the new dataset
file_path = 'disease_diagnosis.csv'
df = pd.read_csv(file_path)

# Convert to Hugging Face dataset
dataset = Dataset.from_pandas(df)

# Define LLaMA-style prompt format
llama31_prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a medical diagnosis assistant that infers likely conditions from patient vitals. Based on the following symptoms and vital signs, provide a detailed diagnosis and reasoning behind the condition.

<|eot_id|><|start_header_id|>user<|end_header_id|>

Oxygen Saturation: {}%, Heart Rate: {} bpm, Temperature: {}°C

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Please provide your diagnosis and reasoning for the condition.
<|eot_id|>"""

# Formatting function
def formatting_prompts_func(examples):
    # Prepare the inputs with relevant columns
    inputs = [
        f"Oxygen Saturation: {x}%, Heart Rate: {y} bpm, Temperature: {z}°C"
        for x, y, z in zip(
            examples["Oxygen_Saturation_%"],
            examples["Heart_Rate_bpm"],
            examples["Body_Temperature_C"]
        )
    ]

    # Outputs (Diagnosis)
    outputs = [str(d) for d in examples["Diagnosis"]]

    # Now the prompt includes reasoning instructions
    texts = [
        llama31_prompt.format(o2, hr, temp, f"Diagnosis: {diagnosis}. Please explain your reasoning.")
        for (o2, hr, temp), diagnosis in zip(
            zip(examples["Oxygen_Saturation_%"], examples["Heart_Rate_bpm"], examples["Body_Temperature_C"]),
            outputs
        )
    ]

    return {"text": texts}


# Apply prompt formatting
dataset = dataset.map(formatting_prompts_func, batched=True)

# Initialize model
max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
   model_name="unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
   max_seq_length=max_seq_length,
   dtype=dtype,
   load_in_4bit=load_in_4bit
)

# Inject LoRA adapters
model = FastLanguageModel.get_peft_model(
   model,
   r=16,
   target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
   lora_alpha=16,
   lora_dropout=0,
   bias="none",
   use_gradient_checkpointing="unsloth",
   random_state=3407,
   use_rslora=False
)

# Training
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
   model=model,
   tokenizer=tokenizer,
   train_dataset=dataset,
   dataset_text_field="text",
   max_seq_length=max_seq_length,
   dataset_num_proc=2,
   packing=False,
   args=TrainingArguments(
       per_device_train_batch_size=2,
       gradient_accumulation_steps=4,
       warmup_steps=5,
       max_steps=60,
       learning_rate=2e-4,
       fp16=not is_bfloat16_supported(),
       bf16=is_bfloat16_supported(),
       logging_steps=1,
       optim="adamw_8bit",
       weight_decay=0.01,
       lr_scheduler_type="linear",
       seed=3407,
       output_dir="outputs"
   ),
)



# Start Fine-tuning job
trainer_stats = trainer.train()

# Save the model
"""<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.
"""
model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")

# Save to 4bit Q4_0
if True: model.save_pretrained_gguf("model", tokenizer, quantization_method = [ "Q4_0"])

!pip install gdown
!gdown --id <file_id> --output unsloth.Q4_0.gguf

from google.colab import files
files.download('model/unsloth.Q4_0.gguf')

# Commented out IPython magic to ensure Python compatibility.
# Make sure you're in the build directory
# %cd /content/llama.cpp/build

# Build the correct quantizer
!cmake --build . --target quantize-gptq

# Commented out IPython magic to ensure Python compatibility.
# Use llamacpp to quantize from Q8 to Q4
!git clone https://github.com/ggerganov/llama.cpp
# %cd llama.cpp
!make quantize
!./quantize unsloth.Q8_0.gguf unsloth.Q4_K_M.gguf Q4_K_M

!zip -r model.zip model/

FROM ./unsloth.Q8_0.gguf
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|reserved_special_token"

!zip -r file.zip Folder_To_Zip